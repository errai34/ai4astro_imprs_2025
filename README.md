# AI4Astro IMPRS 2025 - Transformer Tutorial

This repository contains materials for a transformer tutorial given at the AI for Astronomy IMPRS 2025 program.

## Overview

This tutorial covers the fundamentals of transformer architectures and their applications in astronomy. The materials include hands-on exercises to build understanding of attention mechanisms, encoder-decoder structures, and practical implementations.

## Contents

- `numpy_transformer_student_fill_in.ipynb` - Interactive Jupyter notebook with fill-in exercises to implement a transformer from scratch using NumPy

## Prerequisites

- Python 3.7+
- NumPy
- Jupyter Notebook
- Basic understanding of linear algebra and neural networks

## Getting Started

1. Clone this repository:
   ```bash
   git clone https://github.com/[username]/ai4astro_imprs_2025.git
   cd ai4astro_imprs_2025
   ```

2. Install required dependencies:
   ```bash
   pip install numpy jupyter matplotlib
   ```

3. Start Jupyter notebook:
   ```bash
   jupyter notebook
   ```

4. Open `numpy_transformer_student_fill_in.ipynb` and follow the exercises

## Tutorial Structure

The tutorial is designed to be hands-on and interactive, covering:

- Attention mechanisms and self-attention
- Multi-head attention
- Positional encoding
- Transformer encoder and decoder blocks
- Complete transformer implementation
- Applications to astronomical data

## Learning Objectives

By the end of this tutorial, participants will:
- Understand the core concepts behind transformer architectures
- Implement attention mechanisms from scratch
- Build a complete transformer model using only NumPy
- Appreciate the power and flexibility of attention-based models for sequence processing

## Resources

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original transformer paper
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - Visual guide to transformers
- [Transformers from Scratch](https://peterbloem.nl/blog/transformers) - Detailed implementation guide

## Contributing

If you find any issues or have suggestions for improvements, please feel free to open an issue or submit a pull request.

## License

This educational material is provided under the MIT License.